
\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.05 in}
\setlength{\evensidemargin}{-0.05 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{9.5 in}
\setlength{\headsep}{0.25 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{amsmath,amsfonts,graphicx}

\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[2]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \mbox{
       \vbox{\vspace{2mm}
         \hbox {\leftline{\Large LECTURE #1: \hfill}}
         \vspace{3mm}
         \hbox {\leftline{\Large #2 \hfill}}
         \vspace{4mm}
         \hrule
         \vspace{3mm}
         \hbox to 6.4in { {H. Xiao \hfill NOVEMBER 2012} }
         \vspace{3mm}}
        }
   \end{center}
   \markboth{LECTURE #1: #2}{LECTURE #1: #2}
   \vspace*{4mm}
}

\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}

\newcommand{\fig}[3]{
    		\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.

\def\Z{{\mathbb Z}}
\def\N{{\mathbb N}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\F{{\mathbb F}}
\def\L{{\mathcal L}}
\def\P{{\mathcal P}}
\def\tr{\mathop{\rm tr}}
\def\im{\mbox{im}}

\newcommand{\V}{{\bf V}}
\newcommand{\W}{{\bf W}}
\newcommand{\U}{{\bf U}}
\newcommand{\ip}[2]{\langle {#1}, {#2} \rangle}

\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

\newenvironment{proof}{{\it Proof.}}{\hfill\rule{2mm}{2mm}}


\begin{document}

\lecture{3}{Lenstra-Lenstra-Lov\'{a}sz lattice basis reduction algorithm}

Recall that a full-rank \emph{lattice} $\Lambda$ is a discrete additive subgroup of $\R^n$ generated by all the integer combinations of $n$ linearly  independent vectors. Such a set of $n$ vectors is called a \emph{basis} of $\Lambda$. And $\Lambda$ is usually expressed as
$$\{c_1b_1+\dots+c_nb_n:c_i\in\Z\mbox{~for~all~}i\},$$
with the coefficients running over all integers. Given a lattice $\Lambda$, there are infinitely many different choices of lattice bases. One can obtain another lattice basis by a unimodular transformation (multiplying the basis vectors by a square matrix with integer entries and determinant plus or minus one). If we put all $n$ basis vectors together and form an $n\times n$ matrix, the magnitude of the determinant is equal to the volume of the \emph{fundamental parallelepiped}
$$\{x_1b_1+\dots+x_nb_n:0\leq x_i<1 \mbox{~for~all~}i\}$$
associated with the basis. Observing that the fundamental parallelepiped of two different lattice bases have the same volume. Hence, it makes sense to define the \emph{determinant} of a lattice as the volume of the fundamental parallelepiped of any basis.

Noticing the similarity between lattice and vector space, an orthogonal basis could be very help for investigating the property of a lattice. Not every lattice has an orthogonal basis, but a basis whose vectors are as orthogonal as possible will still preserve some good properties. A basis like that is said \emph{nearly orthogonal}. One measure of the orthogonality of a basis is called the \emph{orthogonality defect}, which is the ratio of the product of the Euclidean norms of the basis vectors over the determinant of the lattice $\det(\Lambda)$
$$\frac{\lVert b_1\rVert\cdot\lVert b_2\rVert\cdots\lVert b_n\rVert}{\det \Lambda}.$$
By the \underline{Hadamard inequality}, the quantity is larger than or equal to one. The equality holds if and only if the basis vectors are orthogonal. To find a nearly orthogonal basis, we only need to find a basis which minimizes orthogonality defect. A classical theorem of Hermite states that for each $n$ there exists a number $c(n)$ such that every $n$-dimensional lattice $\Lambda$ has a basis $v_1,\dots,v_n$ with
$$\lVert b_1\rVert\cdot\lVert b_2\rVert\cdots\lVert b_n\rVert\leq c(n)\det\Lambda.$$
Hermite also showed that we can take $c(n)=(\frac{4}{3})^{n(n-1)/4}$. So once we find a basis satisfying the inequality above, we can think of this basis as an approximation of an orthogonal basis, and such a nearly orthogonal basis always exists.

In the vector space, we can always apply \underline{Gram-Schmidt process} to get an orthogonal basis. The Gram-Schmidt process is initialized by setting $b_1^\ast=b_1$ and recursively computes
$$b_i^\ast=b_i-\sum_{j=1}^{i-1}\mu_{i,j}b_j^\ast,$$
where $\mu_{i,j}=\frac{\ip{b_i}{b_j^\ast}}{{\lVert b_j^\ast\rVert}^2}$, for $i=2,3,\dots,n$. However, as the coefficients $\mu_{i,j}$ are not integral in general, the Gram-Schmidt process does not yield a lattice basis.

If we define the matrix $B=[b_1 b_2\dots b_n]$, the Gram-Schmidt process yields a QR-decomposition of $B$, where
$Q=[b_1^\ast b_2^\ast \dots b_n^\ast]$ and
$$R=
\begin{bmatrix}
1 & \mu_{21} & \cdots & \mu_{n1}\\
0 &  1 & \cdots  & \mu_{n2}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{bmatrix}.$$


Observing that the coefficients $\mu_{i,j}$ would be all zero, if the basis vectors $b_1,\dots,b_n$ were orthogonal. In this case, the orthogonality defect attains the minimum value 1. This observation motivates the idea called \emph{size-reduction}. We can reduce $\mu_{i,j}$ by subtracting the nearest integer to $\mu_{i,j}$ to make sure $|\mu_{i,j}|\leq 1/2$. By elementary column operations one can change $R$ into a matrix in upper-triangular form, with 1's on the diagonal, and all other entries at most $\frac{1}{2}$ in absolute value. This seems to be the best relaxation we can do.

More formally, size-reduction step can be performed:
\begin{tabbing}
\hspace*{.25in} \= \hspace*{.25in} \= \hspace*{.25in} \= \hspace*{.25in} \= \hspace*{.25in} \=\kill
\>{\bf Size Reduction}\\
\>{\bf for} $i=2$ to $n$ \\
\>\> $b_i=\mu_{i,1}b_1^\ast+\mu_{i,2}b_2^\ast+\dots+\mu_{i,i-1}b_{i-1}^\ast+b_i^\ast$\\
\>\> {\bf for} $j=i-1,i-2,\dots,1$ \\
\>\>\> $m \leftarrow $ nearest integer to $\mu_{i,j}$\\
\>\>\> $b_i \leftarrow b_i-mb_j$\\
\>\> {\bf end} \\
\> {\bf end} 
\end{tabbing}

The second idea in basis reduction is to maintain the \emph{Lov\'{a}sz condition}
$$(\delta-\mu_{i+1,i}^2)\lVert b_i^\ast\rVert^2 \leq \lVert b_{i+1}^\ast\rVert^2,$$
which is designed to upper bound the orthogonality defect. By Pythagorean Theorem, the Gram-Schmidt vectors $b_i^\ast$ can get shorter and shorter. This condition requires that their length can not decrease too quickly. Specifically, for any $1/4<\delta<1$, if we set $\alpha=1/(\delta-\frac{1}{4})$, then Lov\'{a}sz condition implies
$$\lVert b_i^\ast\rVert^2 \leq \alpha\lVert b_{i+1}^\ast\rVert^2.$$
Particularly, $\delta$ is a parameter which is usually taken as $3/4$, then $\alpha=2$. It follows that
\begin{align*}
\lVert b_i \rVert^2 &=  \lVert \mu_{i,1}b_1^\ast+\mu_{i,2}b_2^\ast+\dots+\mu_{i,i-1}b_{i-1}^\ast+b_i^\ast \rVert^2\\
&\leq  \lVert b_i^\ast\rVert^2+\frac{1}{4}\sum_{k=1}^{i-1}\lVert b_k^\ast\rVert^2\\
&\leq \lVert b_i^\ast\rVert^2(1+\frac{1}{4}\sum_{k=1}^{i-1}2^{i-k})\\
&\leq 2^{i-1}\lVert b_i^\ast\rVert^2.
\end{align*}

Notice that $\det \Lambda=|\det B |=|\det Q \cdot \det R |=|\det Q|=\prod_{i=1}^{n}\lVert b_i^\ast\rVert$, then consider the orthogonality defect of a basis satisfying Lov\'{a}sz condition,
\begin{align*}
\frac{\lVert b_1\rVert\cdot\lVert b_2\rVert\cdots\lVert b_n\rVert}{\det \Lambda} &=\frac{\lVert b_1\rVert\cdot\lVert b_2\rVert\cdots\lVert b_n\rVert}{\lVert b_1^\ast\rVert\cdot\lVert b_2^\ast\rVert\cdots\lVert b_n^\ast\rVert}\\
&\leq \prod_{i=1}^{n}2^\frac{i-1}{2}
\end{align*}
\begin{definition}[$\delta$-reduced basis]
A basis $\{b_1, b_2, \dots, b_n\}$ is a $\delta$-reduced basis if
\begin{itemize}
  \item all the non-diagonal entries of R satisfy $|\mu_{i,j}|\leq 1/2$,
  \item for any pair of consecutive vectors $b_i$ and $b_{i+1}$, we have $\delta\lVert\pi_{S_i}(b_i)\rVert^2\leq\lVert\pi_{S_i}(b_{i+1})\rVert^2$,
      where $S_i$ is the orthogonal complement of $\mbox{span}(b_1,\dots,b_{i-1})$, and $\pi_{S_i}$ is the projection operator to $S_i$.
\end{itemize}
\end{definition}

It is fulfilled by a process resembling the Euclidean GCD algorithm: Subtract an integral multiple of the shorter vector from the longer one to get a vectors as short as possible; if the length ordering is broken, we swap the two vectors and repeat, otherwise the procedure ends (this idea is also a generation of Gauss Algorithm to higher dimensions).

\section*{References}
\beginrefs
\bibentry{CW87}{\sc D.~Coppersmith} and {\sc S.~Winograd},
``Matrix multiplication via arithmetic progressions,''
{\it Proceedings of the 19th ACM Symposium on Theory of Computing},
1987, pp.~1--6.
\endrefs

% **** THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:

\end{document}






